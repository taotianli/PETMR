{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from dgl.data import DGLDataset\n",
    "from global_graph_constuction import extract_all_global_graph_feature\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class MyDataset(DGLDataset):\n",
    "    def __init__(self,\n",
    "                 url=None,\n",
    "                 raw_dir=None,\n",
    "                 save_dir=None,\n",
    "                 force_reload=False,\n",
    "                 verbose=False):\n",
    "        super(MyDataset, self).__init__(name='dataset_name',\n",
    "                                        url=url,\n",
    "                                        raw_dir=raw_dir,\n",
    "                                        save_dir=save_dir,\n",
    "                                        force_reload=force_reload,\n",
    "                                        verbose=verbose)\n",
    "\n",
    "    def process(self):\n",
    "        # local_feature_dict = extract_all_local_feature()\n",
    "        global_graph_dict = extract_all_global_graph_feature()\n",
    "        self.graphs = []\n",
    "        self.labels = []\n",
    "        for sub_path in glob.glob('D:/Down/Output/subjects/*'):\n",
    "            roi_signal = global_graph_dict[sub_path][0]\n",
    "            roi_signal[abs(roi_signal) < threshold] = 0\n",
    "            ndata = roi_signal\n",
    "            src = []\n",
    "            dst = []\n",
    "            edata = []\n",
    "            r, w = roi_signal.shape\n",
    "            for i in range(r):\n",
    "                for j in range(w):\n",
    "                    if roi_signal[i, j] > 0:\n",
    "                        src.append(i)\n",
    "                        dst.append(j)\n",
    "                        edata.append([roi_signal[i, j]])\n",
    "            # print(file_idx)\n",
    "            graph = dgl.graph((src, dst))\n",
    "            r, w = roi_signal.shape\n",
    "            graph.edata['w'] = torch.tensor(edata).float()\n",
    "            # print(graph.edata['w'].size())\n",
    "            graph.ndata['w'] = torch.tensor(ndata[0:graph.num_nodes(), :]).float()\n",
    "            # print(len(ndata))\n",
    "            # print(graph.ndata['w'].size())\n",
    "\n",
    "            self.graphs.append(graph)\n",
    "            # self.labels.append(torch.tensor(int(local_feature_dict[sub_path][1])))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphs[idx], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "\n",
    "threshold = 0.8\n",
    "dataset = MyDataset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch.conv import GINConv\n",
    "from dgl.nn.pytorch.glob import SumPooling, AvgPooling, MaxPooling\n",
    "\n",
    "\n",
    "class ApplyNodeFunc(nn.Module):\n",
    "    \"\"\"Update the node feature hv with MLP, BN and ReLU.\"\"\"\n",
    "    def __init__(self, mlp):\n",
    "        super(ApplyNodeFunc, self).__init__()\n",
    "        self.mlp = mlp\n",
    "        self.bn = nn.BatchNorm1d(self.mlp.output_dim)\n",
    "\n",
    "    def forward(self, h):\n",
    "        h = self.mlp(h)\n",
    "        h = self.bn(h)\n",
    "        h = F.relu(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"MLP with linear output\"\"\"\n",
    "    def __init__(self, num_layers, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear_or_not = True  # default is linear model\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        if num_layers < 1:\n",
    "            raise ValueError(\"number of layers should be positive!\")\n",
    "        elif num_layers == 1:\n",
    "            # Linear model\n",
    "            self.linear = nn.Linear(input_dim, output_dim)\n",
    "        else:\n",
    "            # Multi-layer model\n",
    "            self.linear_or_not = False\n",
    "            self.linears = torch.nn.ModuleList()\n",
    "            self.batch_norms = torch.nn.ModuleList()\n",
    "\n",
    "            self.linears.append(nn.Linear(input_dim, hidden_dim))\n",
    "            for layer in range(num_layers - 2):\n",
    "                self.linears.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.linears.append(nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "            for layer in range(num_layers - 1):\n",
    "                self.batch_norms.append(nn.BatchNorm1d((hidden_dim)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.linear_or_not:\n",
    "            # If linear model\n",
    "            return self.linear(x)\n",
    "        else:\n",
    "            # If MLP\n",
    "            h = x\n",
    "            for i in range(self.num_layers - 1):\n",
    "                h = F.relu(self.batch_norms[i](self.linears[i](h)))\n",
    "            return self.linears[-1](h)\n",
    "\n",
    "class GIN(nn.Module):\n",
    "    \"\"\"GIN model\"\"\"\n",
    "    def __init__(self, num_layers, num_mlp_layers, input_dim, hidden_dim,\n",
    "                 output_dim, final_dropout, learn_eps, graph_pooling_type,\n",
    "                 neighbor_pooling_type):\n",
    "\n",
    "        super(GIN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.learn_eps = learn_eps\n",
    "\n",
    "        # List of MLPs\n",
    "        self.ginlayers = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "\n",
    "        for layer in range(self.num_layers - 1):\n",
    "            if layer == 0:\n",
    "                mlp = MLP(num_mlp_layers, input_dim, hidden_dim, hidden_dim)\n",
    "            else:\n",
    "                mlp = MLP(num_mlp_layers, hidden_dim, hidden_dim, hidden_dim)\n",
    "\n",
    "            self.ginlayers.append(\n",
    "                GINConv(ApplyNodeFunc(mlp), neighbor_pooling_type, 0, self.learn_eps))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Linear function for graph poolings of output of each layer\n",
    "        # which maps the output of different layers into a prediction score\n",
    "        self.linears_prediction = torch.nn.ModuleList()\n",
    "\n",
    "        for layer in range(num_layers):\n",
    "            if layer == 0:\n",
    "                self.linears_prediction.append(\n",
    "                    nn.Linear(input_dim, output_dim))\n",
    "            else:\n",
    "                self.linears_prediction.append(\n",
    "                    nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "        self.drop = nn.Dropout(final_dropout)\n",
    "\n",
    "        if graph_pooling_type == 'sum':\n",
    "            self.pool = SumPooling()\n",
    "        elif graph_pooling_type == 'mean':\n",
    "            self.pool = AvgPooling()\n",
    "        elif graph_pooling_type == 'max':\n",
    "            self.pool = MaxPooling()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        # list of hidden representation at each layer (including input)\n",
    "        hidden_rep = [h]\n",
    "\n",
    "        for i in range(self.num_layers - 1):\n",
    "            h = self.ginlayers[i](g, h)\n",
    "            h = self.batch_norms[i](h)\n",
    "            h = F.relu(h)\n",
    "            hidden_rep.append(h)\n",
    "\n",
    "        score_over_layer = 0\n",
    "\n",
    "        # perform pooling over all nodes in each graph in every layer\n",
    "        for i, h in enumerate(hidden_rep):\n",
    "            pooled_h = self.pool(g, h)\n",
    "            score_over_layer += self.drop(self.linears_prediction[i](pooled_h))\n",
    "\n",
    "        return score_over_layer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import dgl\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "\n",
    "\n",
    "class GINDataLoader():\n",
    "    def __init__(self,\n",
    "                 dataset,\n",
    "                 batch_size,\n",
    "                 device,\n",
    "                 collate_fn=None,\n",
    "                 seed=0,\n",
    "                 shuffle=True,\n",
    "                 split_name='rand',\n",
    "                 fold_idx=0,\n",
    "                 split_ratio=0.7):\n",
    "\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self.kwargs = {'pin_memory': True} if 'cuda' in device.type else {}\n",
    "\n",
    "        labels = [l for _, l in dataset]\n",
    "\n",
    "        if split_name == 'fold10':\n",
    "            train_idx, valid_idx = self._split_fold10(\n",
    "                labels, fold_idx, seed, shuffle)\n",
    "        elif split_name == 'rand':\n",
    "            train_idx, valid_idx,test_idx = self._split_rand(\n",
    "                labels, split_ratio, seed, shuffle)\n",
    "            test_sampler = SubsetRandomSampler(test_idx)\n",
    "            self.test_loader = GraphDataLoader(\n",
    "                dataset, sampler=test_sampler,\n",
    "                batch_size=batch_size, collate_fn=collate_fn, **self.kwargs)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "\n",
    "        self.train_loader = GraphDataLoader(\n",
    "            dataset, sampler=train_sampler,\n",
    "            batch_size=batch_size, collate_fn=collate_fn, **self.kwargs)\n",
    "        self.valid_loader = GraphDataLoader(\n",
    "            dataset, sampler=valid_sampler,\n",
    "            batch_size=batch_size, collate_fn=collate_fn, **self.kwargs)\n",
    "\n",
    "\n",
    "    def train_valid_loader(self):\n",
    "        return self.train_loader, self.valid_loader, self.test_loader\n",
    "#         return self.train_loader, self.valid_loader\n",
    "\n",
    "    def _split_fold10(self, labels, fold_idx=0, seed=0, shuffle=True):\n",
    "        ''' 10 flod '''\n",
    "        assert 0 <= fold_idx and fold_idx < 10, print(\n",
    "            \"fold_idx must be from 0 to 9.\")\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=shuffle, random_state=seed)\n",
    "        idx_list = []\n",
    "        for idx in skf.split(np.zeros(len(labels)), labels):    # split(x, y)\n",
    "            idx_list.append(idx)\n",
    "        train_idx, valid_idx = idx_list[fold_idx]\n",
    "\n",
    "        print(\n",
    "            \"train_set : test_set = %d : %d\",\n",
    "            len(train_idx), len(valid_idx))\n",
    "\n",
    "        return train_idx, valid_idx\n",
    "\n",
    "    def _split_rand(self, labels, split_ratio=0.7, seed=0, shuffle=True):\n",
    "        num_entries = len(labels)\n",
    "        indices = list(range(num_entries))\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(indices)\n",
    "        split = int(math.floor(split_ratio * num_entries))\n",
    "        test_split = int(math.floor(0.9 * num_entries))\n",
    "#         添加test_idx\n",
    "        train_idx, valid_idx, test_idx = indices[:split], indices[split:test_split], indices[test_split:]\n",
    "\n",
    "        print(\n",
    "            \"train_set : test_set = %d : %d\",\n",
    "            len(train_idx), len(valid_idx))\n",
    "\n",
    "        return train_idx, valid_idx,test_idx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def train(args, net, trainloader, optimizer, criterion, epoch):\n",
    "    net.train()\n",
    "\n",
    "    running_loss = 0\n",
    "    total_iters = len(trainloader)\n",
    "    bar = list(range(total_iters))\n",
    "\n",
    "    for pos, (graphs, labels) in zip(bar, trainloader):\n",
    "        # batch graphs will be shipped to device in forward part of model\n",
    "        labels = labels.to(device)\n",
    "        graphs = graphs.to(device)\n",
    "        feat = graphs.ndata.pop('w')\n",
    "        outputs = net(graphs, feat)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    running_loss = running_loss / total_iters\n",
    "\n",
    "    return running_loss\n",
    "\n",
    "\n",
    "def eval_net(args, net, dataloader, criterion):\n",
    "    net.eval()\n",
    "\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_tp = 0\n",
    "    total_tn = 0\n",
    "    total_fp = 0\n",
    "    total_fn = 0\n",
    "\n",
    "    for data in dataloader:\n",
    "        graphs, labels = data\n",
    "        graphs = graphs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        feat = graphs.ndata.pop('w')\n",
    "        total += len(labels)\n",
    "        outputs = net(graphs, feat)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total_correct += (predicted == labels.data).sum().item()\n",
    "        tn, fp, fn, tp = confusion_matrix(labels.data,predicted, labels=[0,1]).ravel()\n",
    "        total_tp += tp\n",
    "        total_tn += tn\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "        loss = criterion(outputs, labels)\n",
    "        # crossentropy(reduce=True) for default\n",
    "        total_loss += loss.item() * len(labels)\n",
    "\n",
    "    loss, acc = 1.0*total_loss / total, 1.0*total_correct / total\n",
    "    specificity = total_tn/(total_fp+total_tn)\n",
    "    sensitivity = total_tp/(total_tp+total_fn)\n",
    "    precision = total_tp/(total_tp+total_fp)\n",
    "    net.train()\n",
    "\n",
    "    return loss, acc, sensitivity, specificity, precision\n",
    "\n",
    "seed = 0\n",
    "batch_size = 64\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "num_layers = 4\n",
    "num_mlp_layers = 5\n",
    "dim_nfeats = 23\n",
    "hidden_dim = 32\n",
    "gclasses = 2\n",
    "final_dropout = 0.6\n",
    "learn_eps = False\n",
    "graph_pooling_type = 'sum'\n",
    "neighbor_pooling_type = 'sum'\n",
    "lr = 0.001\n",
    "epochs = 60\n",
    "\n",
    "def main(args, fold_idx):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "    trainloader, validloader, testloader = GINDataLoader(\n",
    "        dataset, batch_size=batch_size, device=device,\n",
    "        seed=seed, shuffle=True,\n",
    "        split_name='rand', fold_idx=fold_idx).train_valid_loader()\n",
    "\n",
    "\n",
    "    model = GIN(\n",
    "        num_layers, num_mlp_layers,\n",
    "        dim_nfeats, hidden_dim, gclasses,\n",
    "        final_dropout,learn_eps,\n",
    "        graph_pooling_type, neighbor_pooling_type).to(device)\n",
    "\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()  # defaul reduce is true\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "\n",
    "#     for epoch, _, _ in zip(tbar, vbar, lrbar):\n",
    "    train_loss_ret, train_acc_ret, train_sens_ret, train_spec_ret, train_prec_ret  = [], [], [], [], []\n",
    "    val_loss_ret, val_acc_ret, val_sens_ret, val_spec_ret, val_prec_ret = [], [], [], [], []\n",
    "    test_loss_ret, test_acc_ret, test_sens_ret, test_spec_ret, test_prec_ret = [], [], [], [], []\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train(args, model, trainloader, optimizer, criterion, epoch)\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss, train_acc, train_sens, train_spec, train_precision= eval_net(\n",
    "            args, model, trainloader, criterion)\n",
    "\n",
    "        print('epoch: {}, train loss: {:.4f}, accuracy: {:.2f}%'.format(epoch+1, train_loss, 100. * train_acc), end='')\n",
    "        train_loss_ret.append(train_loss)\n",
    "        train_acc_ret.append(train_acc)\n",
    "        train_sens_ret.append(train_sens)\n",
    "        train_spec_ret.append(train_spec)\n",
    "        train_prec_ret.append(train_precision)\n",
    "\n",
    "        valid_loss, valid_acc, valid_sens, valid_spec, valid_precision = eval_net(\n",
    "            args, model, validloader, criterion)\n",
    "        print('\\t\\tvalid loss: {:.4f}, accuracy: {:.2f}%'.format(valid_loss, 100. * valid_acc))\n",
    "        print('valid sensitivity {:.2f},specificity {:.2f}, precision {:.2f}'.format(100*train_sens, 100*train_spec, 100*train_precision))\n",
    "        val_loss_ret.append(valid_loss)\n",
    "        val_acc_ret.append(valid_acc)\n",
    "        val_sens_ret.append(valid_sens)\n",
    "        val_spec_ret.append(valid_spec)\n",
    "        val_prec_ret.append(valid_precision)\n",
    "\n",
    "        test_loss, test_acc, test_sens, test_spec, test_precision = eval_net(\n",
    "            args, model, testloader, criterion)\n",
    "        print('test loss: {:.4f}, accuracy: {:.2f}%'.format(test_loss, 100. * test_acc))\n",
    "#         print('valid sensitivity {:.2f},specificity {:.2f}, precision {:.2f}'.format(100*train_sens, 100*train_spec, 100*train_precision))\n",
    "        test_loss_ret.append(test_loss)\n",
    "        test_acc_ret.append(test_acc)\n",
    "        test_sens_ret.append(test_sens)\n",
    "        test_spec_ret.append(test_spec)\n",
    "        test_prec_ret.append(test_precision)\n",
    "\n",
    "    return train_loss_ret, train_acc_ret, train_sens_ret, train_spec_ret, train_prec_ret, val_loss_ret, val_acc_ret, val_sens_ret, val_spec_ret, val_prec_ret, test_loss_ret, test_acc_ret, test_sens_ret, test_spec_ret, test_prec_ret\n",
    "\n",
    "#save the final result\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "test_acc_list = []\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "test_loss_list = []\n",
    "train_sens_list = []\n",
    "val_sens_list = []\n",
    "test_sens_list = []\n",
    "train_spec_list = []\n",
    "val_spec_list = []\n",
    "test_spec_list = []\n",
    "train_prec_list = []\n",
    "val_prec_list = []\n",
    "test_prec_list = []\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     args = Parser(description='GIN').args\n",
    "#     print('show all arguments configuration...')\n",
    "#     print(args)\n",
    "\n",
    "    args = None\n",
    "# for thres_step in range(3):\n",
    "#     threshold = 0.2+thres_step*0.13\n",
    "\n",
    "    for idx in range(10):\n",
    "        threshold=0.8\n",
    "        seed = 0\n",
    "        dataset = MyDataset()\n",
    "        print('This is No. {}  turn'.format(idx+1))\n",
    "#         train_loss_ret, train_acc_ret, val_loss_ret, val_acc_ret = main(args, idx)\n",
    "        train_loss_ret, train_acc_ret, train_sens_ret, train_spec_ret, train_prec_ret, val_loss_ret, val_acc_ret, val_sens_ret, val_spec_ret, val_prec_ret, test_loss_ret, test_acc_ret, test_sens_ret, test_spec_ret, test_prec_ret = main(args, idx)\n",
    "        train_acc_list.append(train_acc_ret[epochs-1])\n",
    "        val_acc_list.append(val_acc_ret[epochs-1])\n",
    "        test_acc_list.append(test_acc_ret[epochs-1])\n",
    "        train_loss_list.append(train_loss_ret[epochs-1])\n",
    "        val_loss_list.append(val_loss_ret[epochs-1])\n",
    "        test_loss_list.append(test_loss_ret[epochs-1])\n",
    "        train_sens_list.append(train_sens_ret[epochs-1])\n",
    "        val_sens_list.append(val_sens_ret[epochs-1])\n",
    "        test_sens_list.append(test_sens_ret[epochs-1])\n",
    "        train_spec_list.append(train_spec_ret[epochs-1])\n",
    "        val_spec_list.append(val_spec_ret[epochs-1])\n",
    "        test_spec_list.append(test_spec_ret[epochs-1])\n",
    "        train_prec_list.append(train_prec_ret[epochs-1])\n",
    "        val_prec_list.append(val_prec_ret[epochs-1])\n",
    "        test_prec_list.append(test_prec_ret[epochs-1])\n",
    "\n",
    "        plt.subplot(211)\n",
    "        plt.plot(list(range(epochs)), train_loss_ret, color='r')\n",
    "        plt.plot(list(range(epochs)), val_loss_ret, color='g')\n",
    "        plt.subplot(212)\n",
    "        plt.plot(list(range(epochs)), train_acc_ret, color='r')\n",
    "        plt.plot(list(range(epochs)), val_acc_ret, color='g')\n",
    "        plt.show()\n",
    "\n",
    "    print('Final result: valid accuracy {:.2f}, valid sensitivity {:.2f}, valid specificity {:.2f}, valid precision {:.2f}'.format(10*sum(val_acc_list), 10*sum(val_sens_list), 10*sum(val_spec_list), 10*sum(val_prec_list)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "data=pd.read_excel(r'C:\\Users\\Administrator\\Desktop\\1_20191021评分卡解析.xlsx')\n",
    "a=data.ix[:,6:].corr()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.subplots(figsize=(9, 9))\n",
    "sns.heatmap(a, annot=True, vmax=1, square=True, cmap=\"Blues\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}